<!-- HTML header for doxygen 1.8.9.1-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.9.1"/>
<title>XMM — Probabilistic Models for Motion Recognition and Mapping: $title</title>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href='http://fonts.googleapis.com/css?family=Merriweather:400,700,400italic,700italic&subset=latin,latin-ext' rel='stylesheet' type='text/css'>
<link href='http://fonts.googleapis.com/css?family=Droid+Sans:400,700' rel='stylesheet' type='text/css'>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { init_search(); });
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
<link href="jdoxygen.css" rel="stylesheet" type="text/css" />
<link href="jtabs.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">XMM — Probabilistic Models for Motion Recognition and Mapping
   &#160;<span id="projectnumber">0.3</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.9.1 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li class="current"><a href="pages.html"><span>Related&#160;Pages</span></a></li>
      <li><a href="modules.html"><span>Modules</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="index.html">XMM — Probabilistic Models for Continuous Motion Recognition and Mapping</a></li>  </ul>
</div>
</div><!-- top -->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#why">Why another HMM Library?</a></li>
<li class="level1"><a href="#fourmodels">Four Models</a></li>
<li class="level1"><a href="#architecture">Architecture</a></li>
<li class="level1"><a href="#maxmubu">Max/MuBu Implementation</a></li>
<li class="level1"><a href="#relatedpubs">Related Publications</a></li>
</ul>
</div>
<div class="textblock"><p>XMM is a portable, cross-platform C++ library that implements Gaussian Mixture Models and Hidden Markov Models for both recognition and regression. The XMM library was developed with interaction as a central constraint and allows for continuous, real-time use of the proposed methods. </p>
<h1><a class="anchor" id="why"></a>
Why another HMM Library?</h1>
<p>Several general machine learning toolkits have become popular over the years, such as Weka in Java, Sckits-Learn in Python, or more recently MLPack in C++. However, none of the above libraries were adapted for the purpose of this thesis. As a matter of fact, most HMM implementations are oriented towards classification and they often only implement offline inference using the Viterbi algorithm.</p>
<p>In speech processing, the <a href="http://htk.eng.cam.ac.uk/">Hidden Markov Model Toolkit (HTK)</a> has now become a standard in Automatic Speech Recognition, and gave birth to a branch oriented towards synthesis, called <a href="http://hts.sp.nitech.ac.jp/">HTS</a>. Both libraries present many features specific to speech synthesis that do not yet match our use-cases in movement and sound processing, and have a really complex structure that does not facilitate embedding.</p>
<p>Above all, we did not find any library explicitly implementing the Hierarchical HMM, nor the regression methods based on GMMs and HMMs. For these reasons, we decided to start of novel implementation of these methods with the following constraints:</p><ul>
<li><b>Real-Time:</b> Inference must be performed in continuously, meaning that the models must update their internal state and prediction at each new observation to allow continuous recognition and generation.</li>
<li><b>Interactive:</b> The library must be compatible with an interactive learning workflow, that allows users to easily define and edit training sets, train models, and evaluate the results through direct interaction. All models must be able to learn from few examples (possibly a single demonstration).</li>
<li><b>Portable:</b> In order to be integrated within various software, platforms, the library must be portable, cross-platform, and lightweight.</li>
</ul>
<p>We chose C++ that is both efficient and easy to integrate within other software and languages such as Max and Python. We now detail the four models that are implemented to date, the architecture of the library as well as the proposed Max/MuBu implementation with several examples. </p>
<h1><a class="anchor" id="fourmodels"></a>
Four Models</h1>
<p>The implemented models are summarized in Table the following table. Each of the four model addresses a different combination of the multimodal and temporal aspects. We implemented two instantaneous models based on Gaussian Mixture Models and two temporal models with a hierarchical structure, based on an extension of the basic Hidden Markov Model (HMM) formalism. </p><table class="doxtable">
<tr>
<th>\ </th><th>Movement </th><th>Multimodal  </th></tr>
<tr>
<td>Instantaneous </td><td>Gaussian Mixture Model (GMM) </td><td>Gaussian Mixture Regression (GMR) </td></tr>
<tr>
<td>Temporal </td><td>Hierarchical Hidden Markov Model(HHMM) </td><td>Multimodal Hierarchical Hidden Markov Model(MHMM) </td></tr>
</table>
<ul>
<li><b>Gaussian Mixture Models (GMMs)</b> are instantaneous movement models. The input data associated to a class defined by the training sets is abstracted by a mixture (i.e. a weighted sum) of Gaussian distributions. This representation allows recognition in the <em>performance</em> phase: for each input frame the model calculates the likelihood of each class (Figure 1 (<b>a</b>)).</li>
<li><b>Gaussian Mixture Regression (GMR)</b> are a straightforward extension of Gaussian Mixture Models used for regression. Trained with multimodal data, GMR allows for predicting the features of one modality (e.g. sound) from the features of another (e.g. movement) through non-linear regression between both feature sets (Figure 1 (<b>b</b>)).</li>
<li><b>Hierarchical HMM (HHMM)</b> integrates a high-level structure that governs the transitions between classical HMM structures representing the temporal evolution of &mdash; low-level &mdash; movement segments. In the <em>performance</em> phase of the system, the hierarchical model estimates the likeliest gesture according to the transitions defined by the user. The system continuously estimates the likelihood for each model, as well as the time progression within the original training phrases (Figure 1 (<b>c</b>)).</li>
<li><b>Multimodal Hierarchical HMM (MHMM)</b> allows for predicting a stream of sound parameters from a stream of movement features. It simultaneously takes into account the temporal evolution of movement and sound as well as their dynamic relationship according to the given example phrases. In this way, it guarantees the temporal consistency of the generated sound, while realizing the trained temporal movement-sound mappings (Figure 1 (<b>d</b>)).</li>
</ul>
<div class="image">
<img src="xmm_models.jpg" alt="xmm_models.jpg"/>
<div class="caption">
Figure 1: Schematic Representation of the 4 implemented models</div></div>
 <h1><a class="anchor" id="architecture"></a>
Architecture</h1>
<p>Our implementation has a particular attention to the interactive training procedure, and to the respect of the real-time constraints of the <em>performance</em> mode. The library is built upon four components representing phrases, training sets, models and model groups, as represented on Figure 2. A phrase is a multimodal data container used to store training examples. A training set is used to aggregate phrases associated with labels. It provides a set of function for interactive recording, editing and annotation of the phrases. Each instance of a model is connected to a training set that provides access to the training phrases. Performance functions are designed for real-time usage, updating the internal state of the model and the results for each new observation of a new movement. The library is portable and cross-platform. It defines a specific format for exchanging trained models, and provides Python bindings for scripting purpose or offline processing. </p><div class="image">
<img src="xmm_architecture.jpg" alt="xmm_architecture.jpg"/>
<div class="caption">
Figure 2: Architecture of the XMM library</div></div>
 <h1><a class="anchor" id="maxmubu"></a>
Max/MuBu Implementation</h1>
<p>The models are integrated with the <a href="http://ismm.ircam.fr/mubu/">MuBu</a> environment within <a href="http://cycling74.com/">Cycling 74 Max</a> that provides a consistent framework for motion/sound feature extraction and pre-processing; interactive recording, editing, and annotation of the training sets; and interactive sound synthesis. MuBu is freely available on <a href="http://forumnet.ircam.fr/product/mubu/">Ircam's Forumnet</a>.</p>
<p>Max is a visual programming environment dedicated to music and interactive media. We provide an implementation of our library as a set of Max externals and abstractions articulated around the <em>MuBu</em> collection of objects developed at Ircam. Training sets are built using <em>MuBu</em>, a generic container designed to store and process multimodal data such as audio, motion tracking data, sound descriptors, markers, etc. Each training phrase is stored in a buffer of the container, and movement and sound parameters are recorded into separate tracks of each buffer. Markers can be used to specify regions of interest within the recorded examples. Phrases are labeled using the markers or as an attribute of the buffer. This structure allows users to quickly record, modify, and annotate the training examples. Training sets are thus autonomous and can be used to train several models.</p>
<p>Each model can be instantiated as a max object referring to a MuBu container that defines its training set. For training, the model connects to the container and transfers the training examples to its internal representation of phrases. The parameters of the model can be set manually as attributes of the object, such as the number of Gaussian components in the case of a GMM, or the number of states in the case of a HMM. The training is performed in background.</p>
<p>For performance, each object processes an input stream of movement features and updates the results with the same rate. For movement models, the object output the list of likelihoods, complemented with the parameters estimated for each class, such as the time progression in the case of a temporal model, or the weight of each Gaussian component in the case of a GMM. For multimodal models, the object also outputs the most probable sound parameters estimated by the model, that can be directly used to drive the sound synthesis. </p>
<h1><a class="anchor" id="relatedpubs"></a>
Related Publications</h1>
<ul>
<li>J. Françoise, N. Schnell, R. Borghesi, and F. Bevilacqua, Probabilistic Models for Designing Motion and Sound Relationships. In Proceedings of the 2014 International Conference on New Interfaces for Musical Expression, NIME’14, London, UK, 2014. <a href="http://julesfrancoise.com/blog/wp-content/uploads/2014/06/Fran%C3%A7oise-et-al.-2014-Probabilistic-Models-for-Designing-Motion-and-Sound-Relationships.pdf?1ce945">Download</a></li>
<li>J. Françoise, N. Schnell, and F. Bevilacqua, A Multimodal Probabilistic Model for Gesture-based Control of Sound Synthesis. In Proceedings of the 21st ACM international conference on Multimedia (MM’13), Barcelona, Spain, 2013. <a href="http://architexte.ircam.fr/textes/Francoise13b/index.pdf">Download</a></li>
</ul>
<center>Prev: <a class="el" href="index.html#mainpage">Home</a> | Next: <a class="el" href="_compilation.html">Compilation and Usage</a>.</center> </div></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated on Wed Mar 11 2015 21:13:13 for XMM — Probabilistic Models for Motion Recognition and Mapping by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.9.1
</small></address>
</body>
</html>
